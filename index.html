<!DOCTYPE html>
<html>
<head>
	<style type="text/css">
		p.bottommargin {margin-bottom: 1cm}
	</style>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <title>空气刘海</title>
</head>
<body>
<!-- <h1>Hello World</h1> -->
<!-- <p>I'm hosted with GitHub Pages.</p> -->
<h1>HTTP 的长连接和短连接</h1>

<br \>
<hr \>
<h3><a href="https://www.zhihu.com/question/19997004/answer/14600670">为什么很多网站的内容储存用别的域名？有什么好处？</a></h3>

<h4>Answer: </h4>
<p>作者：杜传赢</p>
<p>来源：知乎</p>
<p class="bottommargin">著作权归作者所有，转载请联系作者获得授权。</p>

<p>三个最主流的原因: </p>
<p>1. CDN 缓存更方便</p>
<p>2. 突破浏览器并发限制 (你随便挑一个 G 家的 <a href="https://lh4.googleusercontent.com/-si4dh2myPWk/T81YkSi__AI/AAAAAAAAQ5o/LlwbBRpp58Q/w497-h373/IMG_20120603_163233.jpg">url</a>, 把前面的 lh4 换成 lh3, lh6 啥的，都照样能够访问，像地图之类的需要大量并发下载图片的站点，这个非常重要。)</p>
<p>3. Cookieless, 节省带宽，尤其是上行带宽 一般比下行要慢。。。</p>

<p>还有另外两个非常规原因: </p>
<p>4. 对于 UGC 的内容和主站隔离，防止不必要的安全问题( 上传 js 窃取主站 cookie 之类的) 。 </p>
<p>正是这个原因要求用户内容的域名必须不是自己主站的子域名，而是一个完全独立的第三方域名。</p>

<p>5. 数据做了划分，甚至切到了不同的物理集群，通过子域名来分流比较省事。这个可能被用的不多。</p>

<p>PS: 关于 Cookie 的问题，带宽是次要的，安全隔离才是主要的。</p>
<p>关于多域名，也不是越多越好，虽然服务器端可以做泛解释，浏览器做 dns 解释也是耗时间的，而且太多域名，如果要走 https 的话，还有要多买证书和部署的问题。</p>
<p></p>
<p></p>
<p></p>
<br \>
<hr \>
<h3><a href="https://www.zhihu.com/question/20474326/answer/15696641">浏览器允许的并发请求资源数是什么意思？</a></h3>
<h4>Answer: </h4>
<p>作者：黄良懿</p>
<p>来源：知乎</p>
<p>著作权归作者所有，转载请联系作者获得授权。</p>

<p>@納米黑客 和 @bombless 的答案分别从前后端答复了。我再补充和整理一下吧。</p>
<p>这个问题实际上涉及非常多的考虑和因此而发生的优化技术：</p>
<p>首先，是基于端口数量和线程切换开销的考虑，浏览器不可能无限量的并发请求，因此衍生出来了并发限制和HTTP/1.1的Keep alive。 所以，IE6/7在HTTP/1.1下的并发才2，但HTTP/1.0却是4。 而随着技术的发展，负载均衡和各类NoSQL的大量应用，基本已经足以应对C10K的问题。 但却并不是每个网站都懂得利用domain hash也就是多域名来加速访问。因此，新的浏览器加大了并发数的限制，但却仍控制在8以内。
后端的保护@bombless 已经说得很全面了，补充一小点就是浏览器即使放弃保护自己，将所有请求一起发给服务器，也很可能会引发服务器的并发阈值控制而被BAN，而另外一个控制在8以内的原因也是keep alive技术的存在使得浏览器复用现有连接和服务器通信比创建新连接的性能要更好一些。</p>

<p>所以，浏览器的并发数其实并不仅仅只是良知的要求，而是双方都需要保护自己的默契，并在可靠的情况下提供更好的性能。</p>

<p class="bottommargin">稍微跑跑题据说有益身心健康。</p>
<p>=================== 我是健康的分割线 ========================</p>
<p>前端技术的逐渐成熟，还衍生了domain hash, cookie free, css sprites, js/css combine, max expires time, loading images on demand等等技术。这些技术的出现和大量使用都和并发资源数有关。</p>

<p>按照普通设计，当网站cookie信息有1 KB、网站首页共150个资源时，用户在请求过程中需要发送150 KB的cookie信息，在512 Kbps的常见上行带宽下，需要长达3秒左右才能全部发送完毕。 尽管这个过程可以和页面下载不同资源的时间并发，但毕竟对速度造成了影响。 而且这些信息在js/css/images/flash等静态资源上，几乎是没有任何必要的。 解决方案是启用和主站不同的域名来放置静态资源，也就是cookie free。
将css放置在页面最上方应该是很自然的习惯，但第一个css内引入的图片下载是有可能堵塞后续的其他js的下载的。而在目前普遍过百的整页请求数的前提下，浏览器提供的仅仅数个并发，对于进行了良好优化甚至是前面有CDN的系统而言，是极大的性能瓶颈。 这也就衍生了domain hash技术来使用多个域名加大并发量（因为浏览器是基于domain的并发控制，而不是page），不过过多的散布会导致DNS解析上付出额外的代价，所以一般也是控制在2-4之间。 这里常见的一个性能小坑是没有机制去确保URL的哈希一致性（即同一个静态资源应该被哈希到同一个域名下），而导致资源被多次下载。
再怎么提速，页面上过百的总资源数也仍然是很可观的，如果能将其中一些很多页面都用到的元素如常用元素如按钮、导航、Tab等的背景图，指示图标等等合并为一张大图，并利用css background的定位来使多个样式引用同一张图片，那也就可以大大的减少总请求数了，这就是css sprites的由来。
全站的js/css原本并不多，其合并技术的产生却是有着和图片不同的考虑。 由于cs/js通常可能对dom布局甚至是内容造成影响，在浏览器解析上，不连贯的载入是会造成多次重新渲染的。因此，在网站变大需要保持模块化来提高可维护性的前提下，js/css combine也就自然衍生了，同时也是minify、compress等对内容进行多余空格、空行、注释的整理和压缩的技术出现的原因。
随着cookie free和domain hash的引入，网站整体的打开速度将会大大的上一个台阶。 这时我们通常看到的问题是大量的请求由于全站公有header/footer/nav等关系，其对应文件早已在本地缓存里存在了，但为了确保这个内容没有发生修改，浏览器还是需要请求一次服务器，拿到一个304 Not Modified才能放心。 一些比较大型的网站在建立了比较规范的发布制度后，会将大部分静态资源的有效期设置为最长，也就是Cache-Control max-age为10年。 这样设置后，浏览器就再也不会在有缓存的前提下去确认文件是否有修改了。 超长的有效期可以让用户在访问曾访问过的网站或网页时，获得最佳的体验。 带来的复杂性则体现在每次对静态资源进行更新时，必须发布为不同的URL来确保用户重新加载变动的资源。
即使是这样做完，仍然还存在着一个很大的优化空间，那就是很多页面浏览量很大，但其实用户直接很大比例直接就跳走了，第一屏以下的内容用户根本就不感兴趣。 对于超大流量的网站如淘宝、新浪等，这个问题尤其重要。 这个时候一般是通过将图片的src标签设置为一个loading或空白的样式，在用户翻页将图片放入可见区或即将放入可见区时再去载入。 不过这个优化其实和并发资源数的关系就比较小了，只是对一些散布不合理，或第一页底部的资源会有一定的帮助。 主要意图还是降低带宽费用。
总的来说，各类技术都是为了能让用户更快的看到页面进行下一步操作，但却不必将宝贵的资源浪费在没有必要的重复请求、不看的内容上。</p>
</body>
</html>
